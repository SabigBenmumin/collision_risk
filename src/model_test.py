"""
YOLOv8 Model Testing & Evaluation Script
‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á model ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö object ‡∏ö‡∏ô‡∏ñ‡∏ô‡∏ô
"""

import os

# Fix OpenMP conflict
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from ultralytics import YOLO
import time
from collections import defaultdict
import json

# Fix multiprocessing issue on Windows
if __name__ == '__main__':
    import multiprocessing
    multiprocessing.freeze_support()

class YOLOModelTester:
    def __init__(self, model_path, test_data_yaml, output_dir="test_results"):
        """
        Initialize YOLO Model Tester
        
        Args:
            model_path: path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á model weights (.pt file)
            test_data_yaml: path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á data.yaml ‡∏Ç‡∏≠‡∏á test dataset
            output_dir: folder ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        """
        self.model = YOLO(model_path)
        self.test_data_yaml = test_data_yaml
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î model ‡∏à‡∏≤‡∏Å: {model_path}")
        print(f"‚úÖ Test data: {test_data_yaml}")
        print(f"‚úÖ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô: {output_dir}")
        
    def run_validation(self, conf=0.25, iou=0.45, split='test'):
        """
        ‡∏£‡∏±‡∏ô validation ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        
        Args:
            conf: confidence threshold (default: 0.25)
            iou: IOU threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NMS (default: 0.45)
            split: dataset split to use ('test', 'val', or 'valid')
        """
        print("\n" + "="*60)
        print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Model Validation")
        print("="*60)
        
        # ‡∏£‡∏±‡∏ô validation
        metrics = self.model.val(
            data=self.test_data_yaml,
            split=split,
            conf=conf,
            iou=iou,
            save_json=True,
            plots=True,
            workers=0,  # ‡∏õ‡∏¥‡∏î multiprocessing ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Windows
            batch=1     # ‡∏•‡∏î batch size ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM
        )
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• metrics ‡∏´‡∏•‡∏±‡∏Å
        self._print_main_metrics(metrics)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        self._save_metrics(metrics, conf, iou)
        
        return metrics
    
    def _print_main_metrics(self, metrics):
        """‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• metrics ‡∏´‡∏•‡∏±‡∏Å‡πÜ"""
        print("\n" + "="*60)
        print("üìä MAIN METRICS")
        print("="*60)
        
        print(f"\nüéØ Overall Performance:")
        print(f"  mAP50     : {metrics.box.map50:.4f}  (‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏Å‡∏•‡πâ 1.0 ‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)")
        print(f"  mAP50-95  : {metrics.box.map:.4f}  (‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏Å‡∏•‡πâ 1.0 ‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)")
        print(f"  Precision : {metrics.box.mp:.4f}  (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)")
        print(f"  Recall    : {metrics.box.mr:.4f}  (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°)")
        
        print(f"\nüìã Per-Class Performance:")
        try:
            # ‡πÅ‡∏™‡∏î‡∏á metrics ‡πÅ‡∏ï‡πà‡∏•‡∏∞ class
            class_names = self.model.names
            maps = metrics.box.maps  # mAP50 ‡πÅ‡∏ï‡πà‡∏•‡∏∞ class
            
            for i, (class_id, class_name) in enumerate(class_names.items()):
                if i < len(maps):
                    print(f"  {class_name:20s}: mAP50 = {maps[i]:.4f}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á per-class metrics: {e}")
    
    def _save_metrics(self, metrics, conf, iou):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metrics ‡πÄ‡∏õ‡πá‡∏ô JSON"""
        results = {
            "model_path": str(self.model.ckpt_path),
            "test_data": str(self.test_data_yaml),
            "confidence_threshold": conf,
            "iou_threshold": iou,
            "metrics": {
                "mAP50": float(metrics.box.map50),
                "mAP50-95": float(metrics.box.map),
                "precision": float(metrics.box.mp),
                "recall": float(metrics.box.mr),
            },
            "per_class_map50": {}
        }
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° per-class metrics
        try:
            class_names = self.model.names
            maps = metrics.box.maps
            for i, (class_id, class_name) in enumerate(class_names.items()):
                if i < len(maps):
                    results["per_class_map50"][class_name] = float(maps[i])
        except:
            pass
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON
        output_file = self.output_dir / "metrics_summary.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metrics summary ‡∏ó‡∏µ‡πà: {output_file}")
    
    def test_inference_speed(self, test_image_dir, num_images=50):
        """
        ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ inference
        
        Args:
            test_image_dir: folder ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û test
            num_images: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
        """
        print("\n" + "="*60)
        print("‚ö° ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß Inference Speed")
        print("="*60)
        
        image_paths = list(Path(test_image_dir).glob("*.jpg"))[:num_images]
        
        if len(image_paths) == 0:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô folder ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏")
            return
        
        times = []
        
        for img_path in image_paths:
            img = cv2.imread(str(img_path))
            
            start_time = time.time()
            results = self.model.predict(source=img, conf=0.65, verbose=False)
            end_time = time.time()
            
            inference_time = (end_time - start_time) * 1000  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô ms
            times.append(inference_time)
        
        avg_time = np.mean(times)
        avg_fps = 1000 / avg_time
        
        print(f"\nüìà Inference Speed Results (‡∏à‡∏≤‡∏Å {len(image_paths)} ‡∏£‡∏π‡∏õ):")
        print(f"  Average Time : {avg_time:.2f} ms")
        print(f"  Average FPS  : {avg_fps:.2f} fps")
        print(f"  Min Time     : {np.min(times):.2f} ms")
        print(f"  Max Time     : {np.max(times):.2f} ms")
        
        if avg_fps >= 25:
            print(f"  ‚úÖ Real-time ready! (‚â•25 fps)")
        else:
            print(f"  ‚ö†Ô∏è ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö real-time (‡∏Ñ‡∏ß‡∏£ ‚â•25 fps)")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        speed_results = {
            "num_images_tested": len(image_paths),
            "average_time_ms": float(avg_time),
            "average_fps": float(avg_fps),
            "min_time_ms": float(np.min(times)),
            "max_time_ms": float(np.max(times))
        }
        
        with open(self.output_dir / "inference_speed.json", 'w') as f:
            json.dump(speed_results, f, indent=2)
        
        return avg_fps
    
    def visualize_predictions(self, test_image_dir, num_samples=10, conf=0.65):
        """
        ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û predictions ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö ground truth
        
        Args:
            test_image_dir: folder ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
            num_samples: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á
            conf: confidence threshold
        """
        print("\n" + "="*60)
        print("üñºÔ∏è  ‡∏™‡∏£‡πâ‡∏≤‡∏á Visualization")
        print("="*60)
        
        image_paths = list(Path(test_image_dir).glob("*.jpg"))[:num_samples]
        
        if len(image_paths) == 0:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô folder ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏")
            return
        
        vis_dir = self.output_dir / "visualizations"
        vis_dir.mkdir(exist_ok=True)
        
        for idx, img_path in enumerate(image_paths):
            img = cv2.imread(str(img_path))
            
            # Predict
            results = self.model.predict(source=img, conf=conf, verbose=False)
            
            # Plot results
            annotated = results[0].plot()
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ
            output_path = vis_dir / f"prediction_{idx+1}.jpg"
            cv2.imwrite(str(output_path), annotated)
            
            print(f"  ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà {idx+1}/{len(image_paths)}: {output_path.name}")
        
        print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ visualizations ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô: {vis_dir}")
    
    def generate_report(self, metrics, avg_fps=None):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        """
        print("\n" + "="*60)
        print("üìù ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•")
        print("="*60)
        
        report_path = self.output_dir / "evaluation_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("YOLO MODEL EVALUATION REPORT\n")
            f.write("‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Object ‡∏ö‡∏ô‡∏ñ‡∏ô‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Collision Risk\n")
            f.write("="*70 + "\n\n")
            
            f.write("1. OVERALL PERFORMANCE\n")
            f.write("-"*70 + "\n")
            f.write(f"mAP50      : {metrics.box.map50:.4f} (IoU threshold = 0.5)\n")
            f.write(f"mAP50-95   : {metrics.box.map:.4f} (IoU threshold = 0.5-0.95)\n")
            f.write(f"Precision  : {metrics.box.mp:.4f}\n")
            f.write(f"Recall     : {metrics.box.mr:.4f}\n")
            
            if avg_fps:
                f.write(f"\n2. INFERENCE SPEED\n")
                f.write("-"*70 + "\n")
                f.write(f"Average FPS: {avg_fps:.2f} fps\n")
                if avg_fps >= 25:
                    f.write("Status     : ‚úÖ Real-time ready\n")
                else:
                    f.write("Status     : ‚ö†Ô∏è ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö real-time\n")
            
            f.write(f"\n3. PER-CLASS PERFORMANCE\n")
            f.write("-"*70 + "\n")
            
            try:
                class_names = self.model.names
                maps = metrics.box.maps
                for i, (class_id, class_name) in enumerate(class_names.items()):
                    if i < len(maps):
                        f.write(f"{class_name:20s}: mAP50 = {maps[i]:.4f}\n")
            except:
                f.write("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á per-class metrics\n")
            
            f.write(f"\n4. RECOMMENDATIONS FOR STAKEHOLDERS\n")
            f.write("-"*70 + "\n")
            
            # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏≤‡∏° metrics
            if metrics.box.map50 >= 0.8:
                f.write("‚úÖ Model ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\n")
            elif metrics.box.map50 >= 0.6:
                f.write("‚ö†Ô∏è Model ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n")
            else:
                f.write("‚ùå Model ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å\n")
            
            if metrics.box.mr < 0.7:
                f.write("‚ö†Ô∏è Recall ‡∏ï‡πà‡∏≥ - model ‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö object ‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß\n")
                f.write("   ‚Üí ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏≤‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢!\n")
            
            f.write("\n" + "="*70 + "\n")
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà: {report_path}")

def main():
    """
    ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    """
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë        YOLOv8 Model Testing & Evaluation                     ‚ïë
    ‚ïë        ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Collision Risk Analysis                        ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # ========== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ==========
    MODEL_PATH = r"models/best.pt"              # path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á model weights
    TEST_DATA_YAML = r"TestDataset5classes/data.yaml"               # path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á data.yaml
    TEST_IMAGE_DIR = r"TestDataset5classes/test/images"             # folder ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û test
    OUTPUT_DIR = "test_results"                # folder ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    
    CONF_THRESHOLD = 0.65                      # confidence threshold (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô video.py)
    IOU_THRESHOLD = 0.45                       # IOU threshold
    # ===================================
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á tester object
    tester = YOLOModelTester(
        model_path=MODEL_PATH,
        test_data_yaml=TEST_DATA_YAML,
        output_dir=OUTPUT_DIR
    )
    
    # 1. ‡∏£‡∏±‡∏ô validation
    print("\nüîç ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: Validation")
    metrics = tester.run_validation(
        conf=CONF_THRESHOLD, 
        iou=IOU_THRESHOLD,
        split='test'  # ‡πÉ‡∏ä‡πâ 'test' split ‡πÅ‡∏ó‡∏ô 'val'
    )
    
    # 2. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    print("\nüîç ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: Speed Test")
    avg_fps = tester.test_inference_speed(
        test_image_dir=TEST_IMAGE_DIR,
        num_images=50
    )
    
    # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á visualizations
    print("\nüîç ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: Visualizations")
    tester.visualize_predictions(
        test_image_dir=TEST_IMAGE_DIR,
        num_samples=10,
        # conf=CONF_THRESHOLD
    )
    
    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
    print("\nüîç ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: Generate Report")
    tester.generate_report(metrics, avg_fps)
    
    print("\n" + "="*60)
    print("‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô folder:", OUTPUT_DIR)
    print("="*60)
    print("\n‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô:")
    print("  üìÑ metrics_summary.json    - ‡∏™‡∏£‡∏∏‡∏õ metrics ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    print("  üìÑ inference_speed.json    - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ inference")
    print("  üìÑ evaluation_report.txt   - ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•")
    print("  üìÅ visualizations/         - ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û predictions")
    print()

if __name__ == "__main__":
    main()